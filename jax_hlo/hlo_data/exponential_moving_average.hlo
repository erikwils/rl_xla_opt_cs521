module @jit_exponential_moving_average attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
  func.func public @main(%arg0: tensor<6xf32> {mhlo.layout_mode = "default"}, %arg1: tensor<f32> {mhlo.layout_mode = "default"}) -> (tensor<6xf32> {jax.result_info = "", mhlo.layout_mode = "default"}) {
    %0 = stablehlo.slice %arg0 [0:1] : (tensor<6xf32>) -> tensor<1xf32>
    %1 = stablehlo.reshape %0 : (tensor<1xf32>) -> tensor<f32>
    %2 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5xf32>
    %c = stablehlo.constant dense<0> : tensor<i32>
    %4:5 = stablehlo.while(%iterArg = %2, %iterArg_0 = %arg1, %iterArg_1 = %c, %iterArg_2 = %1, %iterArg_3 = %3) : tensor<5xf32>, tensor<f32>, tensor<i32>, tensor<f32>, tensor<5xf32>
     cond {
      %c_4 = stablehlo.constant dense<5> : tensor<i32>
      %7 = stablehlo.compare  LT, %iterArg_1, %c_4,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
      stablehlo.return %7 : tensor<i1>
    } do {
      %c_4 = stablehlo.constant dense<0> : tensor<i32>
      %7 = stablehlo.compare  LT, %iterArg_1, %c_4,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %8 = stablehlo.convert %iterArg_1 : tensor<i32>
      %c_5 = stablehlo.constant dense<5> : tensor<i32>
      %9 = stablehlo.add %8, %c_5 : tensor<i32>
      %10 = stablehlo.select %7, %9, %iterArg_1 : tensor<i1>, tensor<i32>
      %11 = stablehlo.dynamic_slice %iterArg, %10, sizes = [1] : (tensor<5xf32>, tensor<i32>) -> tensor<1xf32>
      %12 = stablehlo.reshape %11 : (tensor<1xf32>) -> tensor<f32>
      %13:2 = func.call @None(%iterArg_0, %iterArg_2, %12) : (tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>)
      %14 = stablehlo.broadcast_in_dim %13#1, dims = [] : (tensor<f32>) -> tensor<1xf32>
      %c_6 = stablehlo.constant dense<0> : tensor<i32>
      %15 = stablehlo.compare  LT, %iterArg_1, %c_6,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %16 = stablehlo.convert %iterArg_1 : tensor<i32>
      %c_7 = stablehlo.constant dense<5> : tensor<i32>
      %17 = stablehlo.add %16, %c_7 : tensor<i32>
      %18 = stablehlo.select %15, %17, %iterArg_1 : tensor<i1>, tensor<i32>
      %19 = stablehlo.dynamic_update_slice %iterArg_3, %14, %18 : (tensor<5xf32>, tensor<1xf32>, tensor<i32>) -> tensor<5xf32>
      %c_8 = stablehlo.constant dense<1> : tensor<i32>
      %20 = stablehlo.add %iterArg_1, %c_8 : tensor<i32>
      stablehlo.return %iterArg, %iterArg_0, %20, %13#0, %19 : tensor<5xf32>, tensor<f32>, tensor<i32>, tensor<f32>, tensor<5xf32>
    }
    %5 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1xf32>
    %6 = stablehlo.concatenate %5, %4#4, dim = 0 : (tensor<1xf32>, tensor<5xf32>) -> tensor<6xf32>
    return %6 : tensor<6xf32>
  }
  func.func private @None(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
    %0 = stablehlo.convert %arg0 : tensor<f32>
    %1 = stablehlo.multiply %0, %arg2 : tensor<f32>
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>
    %2 = stablehlo.subtract %cst, %arg0 : tensor<f32>
    %3 = stablehlo.convert %2 : tensor<f32>
    %4 = stablehlo.multiply %3, %arg1 : tensor<f32>
    %5 = stablehlo.add %1, %4 : tensor<f32>
    return %5, %5 : tensor<f32>, tensor<f32>
  }
}
